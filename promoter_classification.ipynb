{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eBr00ffrSPkg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lKKTbRrgZxGB",
        "metadata": {},
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be793d71-48fc-46e1-d4ac-dc5a66711a36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "samira1992_promoter_or_not_bioinformatics_dataset_path = kagglehub.dataset_download('samira1992/promoter-or-not-bioinformatics-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-07T02:11:53.88913Z",
          "iopub.status.busy": "2025-02-07T02:11:53.888616Z",
          "iopub.status.idle": "2025-02-07T02:11:54.129757Z",
          "shell.execute_reply": "2025-02-07T02:11:54.128706Z",
          "shell.execute_reply.started": "2025-02-07T02:11:53.889093Z"
        },
        "id": "rossp3OPZxGB",
        "metadata": {},
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import sys\n",
        "import pandas as pd\n",
        "import re\n",
        "import timeit\n",
        "import io\n",
        "import IPython.display as display\n",
        "from PIL import Image\n",
        "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler,\n",
        "                              SequentialSampler)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-06T23:48:30.116167Z",
          "iopub.status.busy": "2025-02-06T23:48:30.115845Z",
          "iopub.status.idle": "2025-02-06T23:48:30.121911Z",
          "shell.execute_reply": "2025-02-06T23:48:30.120585Z",
          "shell.execute_reply.started": "2025-02-06T23:48:30.116143Z"
        },
        "id": "Uy1YSqGiZxGB",
        "metadata": {},
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def load_data_kaggle():\n",
        "\tpromoter_ = pd.read_csv('/kaggle/input/promoter-or-not-bioinformatics-dataset/promoter.csv')\n",
        "\tnon_promoter_ = pd.read_csv('/kaggle/input/promoter-or-not-bioinformatics-dataset/non_promoter.csv')\n",
        "\tpatterns_table_ = pd.read_csv('/kaggle/input/promoter-or-not-bioinformatics-dataset/bendability.tsv', header=None, names=['codon','score'], sep='\\t')\n",
        "\tpromoter_ = promoter_.to_numpy()\n",
        "\tnon_promoter_ = non_promoter_.to_numpy()\n",
        "\n",
        "\treturn promoter_, non_promoter_, patterns_table_\n",
        "\n",
        "def load_data_local():\n",
        "\tpromoter_ = pd.read_csv(os.path.join(samira1992_promoter_or_not_bioinformatics_dataset_path, 'promoter.csv'))\n",
        "\tnon_promoter_ = pd.read_csv(os.path.join(samira1992_promoter_or_not_bioinformatics_dataset_path, 'non_promoter.csv'))\n",
        "\tpatterns_table_ = pd.read_csv(os.path.join(samira1992_promoter_or_not_bioinformatics_dataset_path, 'bendability.tsv'), header=None, names=['codon','score'], sep='\\t')\n",
        "\tpromoter_ = promoter_.to_numpy()\n",
        "\tnon_promoter_ = non_promoter_.to_numpy()\n",
        "\n",
        "\treturn promoter_, non_promoter_, patterns_table_\n",
        "\n",
        "def tokenize_stride1(sequences, max_length = 250, k=3):\n",
        "    adjusted_seqs = [seq[:max_length].ljust(max_length, \"N\").lower() for seq in sequences.flatten()]\n",
        "    return np.array([[seq[i:i+k] for i in range(len(seq) - k + 1)] for seq in adjusted_seqs])\n",
        "\n",
        "def seq_encoding(sequences_kmerized, codons):\n",
        "\n",
        "    encoded_codons=[]\n",
        "    for codon in sequences_kmerized:\n",
        "        if codon not in codons.keys():\n",
        "          if isinstance(codons['atg'], np.ndarray):\n",
        "            encoded_codons.append(np.zeros(64))\n",
        "          else:\n",
        "            encoded_codons.append(0)\n",
        "\n",
        "        else:\n",
        "            encoded_codons.append(codons[codon])\n",
        "\n",
        "    if isinstance(encoded_codons[0], np.ndarray):\n",
        "      return np.array(encoded_codons).T.astype(np.float32)\n",
        "    else:\n",
        "      return np.array(encoded_codons).astype(np.int32)\n",
        "\n",
        "def data_loader(train_inputs, val_inputs, train_labels, val_labels,\n",
        "                batch_size=10):\n",
        "    \"\"\"\n",
        "    Convert train and validation sets to torch.Tensors and load them to\n",
        "    DataLoader.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert data type to torch.Tensor\n",
        "    train_inputs, val_inputs, train_labels, val_labels =\\\n",
        "    tuple(torch.tensor(data) for data in\n",
        "          [train_inputs, val_inputs, train_labels, val_labels])\n",
        "\n",
        "    # Create DataLoader for training data\n",
        "    train_data = TensorDataset(train_inputs, train_labels)\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "    # Create DataLoader for validation data\n",
        "    val_data = TensorDataset(val_inputs, val_labels)\n",
        "    val_sampler = SequentialSampler(val_data)\n",
        "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "\n",
        "    return train_dataloader, val_dataloader\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, embedding_dim=16, num_codons=65, params = [128,6,3,1,(3, 2, 1),128,10,6,3,1,(3,2,1),70]):  #num of codons + unknown\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(num_codons, embedding_dim)\n",
        "\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Conv1d(in_channels=embedding_dim,\n",
        "                out_channels=params[0],\n",
        "                kernel_size=params[1],\n",
        "                stride=params[2],\n",
        "                padding=params[3]),\n",
        "        nn.ReLU(),\n",
        "        #nn.BatchNorm1d(params[1]),\n",
        "        nn.MaxPool1d(*params[4]),\n",
        "        nn.Dropout(p=0.5),\n",
        "\n",
        "        nn.Conv1d(in_channels=params[5],\n",
        "                  out_channels=params[6],\n",
        "                  kernel_size=params[7],\n",
        "                  stride=params[8],\n",
        "                  padding=params[9]),\n",
        "        nn.ReLU(),\n",
        "        #nn.BatchNorm1d(params[7]),\n",
        "        nn.MaxPool1d(*params[10]),\n",
        "        nn.Flatten(),\n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.Linear(params[11], 1),\n",
        "        nn.Sigmoid())\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(f\"Input shape: {x.shape}\")  # Debugging: Print input shape\n",
        "        #print(f\"Input shape: {x.shape}\")  # Debugging: Print input shape\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x = torch.transpose(x, 1, 2)\n",
        "\n",
        "        #for layer in self.linear_relu_stack:\n",
        "        #     print(f\"Before {layer.__class__.__name__}: {x.shape}\")\n",
        "        #     x = layer(x)\n",
        "        #     print(f\"After {layer.__class__.__name__}: {x.shape}\")\n",
        "\n",
        "        return self.linear_relu_stack(x)\n",
        "\n",
        "def plot_metrics(train_loss, test_loss, test_acc, precision, recall, auc):\n",
        "\n",
        "    epochs = range(1, len(train_loss) + 1)\n",
        "\n",
        "    plots = []\n",
        "\n",
        "    # Create a figure\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(12, 4))\n",
        "\n",
        "    # Plot loss curve\n",
        "    axes[0].plot(epochs, train_loss, label='Train Loss')\n",
        "    axes[0].plot(epochs, test_loss, label='Test Loss')\n",
        "    axes[0].set_xlabel('Epochs')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].set_title('Loss Curve')\n",
        "    axes[0].set_ylim(0,1)\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Plot accuracy curve\n",
        "    axes[1].plot(epochs, test_acc, label='Test Accuracy')\n",
        "    axes[1].set_xlabel('Epochs')\n",
        "    axes[1].set_ylabel('Accuracy')\n",
        "    axes[1].set_title('Accuracy Curve')\n",
        "    axes[1].set_ylim(0, 1)\n",
        "    axes[1].legend()\n",
        "\n",
        "    # Plot precision-recall curve\n",
        "    axes[2].plot(recall, precision, label='Precision-Recall Curve')\n",
        "    axes[2].set_xlabel('Recall')\n",
        "    axes[2].set_ylabel('Precision')\n",
        "    axes[2].set_title('Precision-Recall Curve')\n",
        "    axes[2].set_ylim(0, 1)\n",
        "    axes[2].set_xlim(0, 1)\n",
        "    axes[2].legend()\n",
        "\n",
        "    auc_roc, (fpr, tpr) = auc\n",
        "    axes[3].plot(fpr, tpr, label=f\"AUC={auc_roc:.4f}\")\n",
        "    axes[3].set_xlabel('FPR')\n",
        "    axes[3].set_ylabel('TPR')\n",
        "    axes[3].set_title('AUC-ROC Curve')\n",
        "    axes[3].set_ylim(0, 1)\n",
        "    axes[3].set_xlim(0, 1)\n",
        "    axes[3].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plots to list\n",
        "\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf, format='png')\n",
        "    buf.seek(0)\n",
        "    plt.close(fig)\n",
        "\n",
        "    return buf\n",
        "\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, batch_size = 64):\n",
        "\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    losses = []\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    train_loss_value, correct = 0, 0\n",
        "\n",
        "    for i, batch in enumerate(dataloader, 0):\n",
        "        # Compute prediction and loss\n",
        "        X, y = batch[0].to(device), batch[1].to(device).float()\n",
        "\n",
        "        #print(f\"Inputs dtype: {X.dtype}, Labels dtype: {y.dtype}\")\n",
        "        #print(f\"Inputs length: {len(X)}, Labels length: {len(y)}\")\n",
        "\n",
        "        pred = model(X)\n",
        "        pred = pred.view(-1)\n",
        "        train_loss = loss_fn(pred, y)\n",
        "        train_loss_value += train_loss.item()\n",
        "\n",
        "        # Backpropagation\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            loss_batch100, current = train_loss.item(), i * batch_size + len(X)\n",
        "            print(f\"Train loss 100th: {loss_batch100:>0.2f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    # calculate per epoch\n",
        "    train_loss_value /= num_batches\n",
        "    print(f\"Avg Train Loss: {train_loss_value:>0.2f}\")\n",
        "\n",
        "    return train_loss_value\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn, sig_threshold):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "    all_preds, all_labels, all_probs = [], [], []\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(dataloader, 0):\n",
        "\n",
        "            # Compute prediction and loss\n",
        "            X, y = batch[0].to(device), batch[1].to(device).float()\n",
        "            pred = model(X)\n",
        "            pred = pred.squeeze()\n",
        "\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "\n",
        "            predicted_labels = (pred >= sig_threshold).float()\n",
        "            correct += (predicted_labels == y).type(torch.float).sum().item()\n",
        "            all_probs.extend(pred.cpu().numpy())\n",
        "\n",
        "            # Store predictions and true labels for metric calculation\n",
        "            all_preds.extend(predicted_labels.cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    accuracy = correct / size\n",
        "\n",
        "    #Precision/Recall and F1-score calculation\n",
        "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "    auc_roc = roc_auc_score(all_labels, all_probs)\n",
        "    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
        "\n",
        "    print(f\"Test Metrics: \\n Accuracy: {(100 * accuracy):>0.1f}%, \"\n",
        "          f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}, \"\n",
        "          f\"Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "    return test_loss, accuracy, precision, recall, auc_roc, (fpr, tpr)\n",
        "\n",
        "def epochs_loop(train_dataloader, test_dataloader, model, epochs=1, lr=0.01, momentum=0.9, decay=0.0, sig_threshold=0.5):\n",
        "\n",
        "    train_loss_epochs, test_loss_epochs, precision_epochs, recall_epochs, acc_epochs, auc_epochs = [], [], [], [], [], []\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    #optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=decay)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=decay)\n",
        "\n",
        "    # start timer\n",
        "    loop_time = timeit.default_timer()\n",
        "    for t in range(epochs):\n",
        "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "\n",
        "        train_loss = train_loop(dataloader = train_dataloader,\n",
        "                                model = model,\n",
        "                                loss_fn = loss_fn,\n",
        "                                optimizer = optimizer,\n",
        "                                batch_size = batch_size)\n",
        "\n",
        "        test_loss, accuracy, precision, recall, auc_roc, roc_curve_data = test_loop(test_dataloader, model, loss_fn, sig_threshold)\n",
        "\n",
        "        # To plot\n",
        "        train_loss_epochs.append(train_loss)\n",
        "        test_loss_epochs.append(test_loss)\n",
        "        precision_epochs.append(precision)\n",
        "        recall_epochs.append(recall)\n",
        "        acc_epochs.append(accuracy)\n",
        "        # stop timer\n",
        "        print('Time {}'.format(np.round((timeit.default_timer() - loop_time),2)))\n",
        "\n",
        "    plot = plot_metrics(train_loss=train_loss_epochs,\n",
        "                        test_loss=test_loss_epochs,\n",
        "                        test_acc=acc_epochs,\n",
        "                        precision=precision_epochs,\n",
        "                        recall=recall_epochs,\n",
        "                        auc=(auc_roc, roc_curve_data))\n",
        "\n",
        "    return train_loss_epochs, test_loss_epochs, plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-07T02:12:05.806721Z",
          "iopub.status.busy": "2025-02-07T02:12:05.80624Z",
          "iopub.status.idle": "2025-02-07T02:12:43.764693Z",
          "shell.execute_reply": "2025-02-07T02:12:43.763422Z",
          "shell.execute_reply.started": "2025-02-07T02:12:05.80668Z"
        },
        "id": "ENjV6hREZxGC",
        "metadata": {},
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#promoter, non_promoter, codon_score = load_data_kaggle()\n",
        "\n",
        "promoter, non_promoter, codon_score = load_data_local()\n",
        "\n",
        "codon_score_dict = dict(zip(codon_score['codon'], codon_score['score']))\n",
        "\n",
        "all_raw_sequences = np.concatenate([promoter[0:60000], non_promoter[0:60000]])\n",
        "\n",
        "all_labels = labels = np.concatenate([\n",
        "    np.ones(len(promoter[0:60000]), dtype=int),      #1 promoter\n",
        "    np.zeros(len(non_promoter[0:60000]), dtype=int)  #0 non-promoter\n",
        "])\n",
        "\n",
        "sequences_kmerized = tokenize_stride1(all_raw_sequences)\n",
        "\n",
        "#codon_to_one_hot = {codon: np.eye(64)[i] for i, codon in enumerate(codon_score_dict)}\n",
        "#one_hot_seqs = [seq_encoding(seq, codons=codon_to_one_hot) for seq in sequences_kmerized]\n",
        "\n",
        "codon_to_num = {codon: i+1 for i, codon in enumerate(codon_score_dict)}\n",
        "random_token_seqs = [seq_encoding(seq, codons=codon_to_num) for seq in sequences_kmerized]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-07T02:12:49.003234Z",
          "iopub.status.busy": "2025-02-07T02:12:49.002755Z",
          "iopub.status.idle": "2025-02-07T02:12:49.038177Z",
          "shell.execute_reply": "2025-02-07T02:12:49.037101Z",
          "shell.execute_reply.started": "2025-02-07T02:12:49.003198Z"
        },
        "id": "wWfK-XNsZxGD",
        "metadata": {},
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
        "    random_token_seqs, all_labels, train_size=0.8, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataloader, test_dataloader = data_loader(train_inputs, test_inputs, train_labels, test_labels, batch_size=64)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-07T02:13:34.73791Z",
          "iopub.status.busy": "2025-02-07T02:13:34.737501Z",
          "iopub.status.idle": "2025-02-07T02:13:34.745255Z",
          "shell.execute_reply": "2025-02-07T02:13:34.744056Z",
          "shell.execute_reply.started": "2025-02-07T02:13:34.73788Z"
        },
        "id": "ZxBLnNK6ZxGD",
        "metadata": {},
        "outputId": "785de1c5-ede7-4bf1-a432-896231959cc3",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Train loss 100th: 0.74  [   64/48000]\n",
            "Train loss 100th: 0.69  [ 6464/48000]\n",
            "Train loss 100th: 0.69  [12864/48000]\n",
            "Train loss 100th: 0.69  [19264/48000]\n",
            "Train loss 100th: 0.69  [25664/48000]\n",
            "Train loss 100th: 0.69  [32064/48000]\n",
            "Train loss 100th: 0.69  [38464/48000]\n",
            "Train loss 100th: 0.67  [44864/48000]\n",
            "Avg Train Loss: 0.69\n",
            "Test Metrics: \n",
            " Accuracy: 71.6%, Precision: 0.7626, Recall: 0.7128, F1-score: 0.7009, Avg loss: 0.646098 \n",
            "\n",
            "Time 27.93\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Train loss 100th: 0.66  [   64/48000]\n",
            "Train loss 100th: 0.66  [ 6464/48000]\n",
            "Train loss 100th: 0.59  [12864/48000]\n",
            "Train loss 100th: 0.61  [19264/48000]\n",
            "Train loss 100th: 0.62  [25664/48000]\n",
            "Train loss 100th: 0.55  [32064/48000]\n",
            "Train loss 100th: 0.62  [38464/48000]\n",
            "Train loss 100th: 0.59  [44864/48000]\n",
            "Avg Train Loss: 0.61\n",
            "Test Metrics: \n",
            " Accuracy: 82.9%, Precision: 0.8438, Recall: 0.8272, F1-score: 0.8263, Avg loss: 0.581722 \n",
            "\n",
            "Time 55.03\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Train loss 100th: 0.61  [   64/48000]\n",
            "Train loss 100th: 0.60  [ 6464/48000]\n",
            "Train loss 100th: 0.57  [12864/48000]\n",
            "Train loss 100th: 0.59  [19264/48000]\n",
            "Train loss 100th: 0.62  [25664/48000]\n",
            "Train loss 100th: 0.56  [32064/48000]\n",
            "Train loss 100th: 0.58  [38464/48000]\n",
            "Train loss 100th: 0.60  [44864/48000]\n",
            "Avg Train Loss: 0.59\n",
            "Test Metrics: \n",
            " Accuracy: 83.2%, Precision: 0.8547, Recall: 0.8302, F1-score: 0.8289, Avg loss: 0.575155 \n",
            "\n",
            "Time 82.02\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Train loss 100th: 0.58  [   64/48000]\n",
            "Train loss 100th: 0.58  [ 6464/48000]\n",
            "Train loss 100th: 0.62  [12864/48000]\n",
            "Train loss 100th: 0.58  [19264/48000]\n",
            "Train loss 100th: 0.56  [25664/48000]\n",
            "Train loss 100th: 0.59  [32064/48000]\n",
            "Train loss 100th: 0.53  [38464/48000]\n",
            "Train loss 100th: 0.63  [44864/48000]\n",
            "Avg Train Loss: 0.58\n",
            "Test Metrics: \n",
            " Accuracy: 85.5%, Precision: 0.8678, Recall: 0.8541, F1-score: 0.8539, Avg loss: 0.568101 \n",
            "\n",
            "Time 119.05\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Train loss 100th: 0.56  [   64/48000]\n",
            "Train loss 100th: 0.61  [ 6464/48000]\n",
            "Train loss 100th: 0.58  [12864/48000]\n",
            "Train loss 100th: 0.61  [19264/48000]\n",
            "Train loss 100th: 0.57  [25664/48000]\n",
            "Train loss 100th: 0.51  [32064/48000]\n",
            "Train loss 100th: 0.57  [38464/48000]\n",
            "Train loss 100th: 0.57  [44864/48000]\n",
            "Avg Train Loss: 0.57\n",
            "Test Metrics: \n",
            " Accuracy: 86.1%, Precision: 0.8708, Recall: 0.8594, F1-score: 0.8594, Avg loss: 0.566442 \n",
            "\n",
            "Time 147.51\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Train loss 100th: 0.56  [   64/48000]\n",
            "Train loss 100th: 0.59  [ 6464/48000]\n",
            "Train loss 100th: 0.54  [12864/48000]\n",
            "Train loss 100th: 0.60  [19264/48000]\n"
          ]
        }
      ],
      "source": [
        "# Creating net and training\n",
        "\n",
        "#Test Decay\n",
        "#mode1 = [0.01, 0.9, 0.0, 30, 64, 0.5]\n",
        "#mode2 = [0.01, 0.9, 1e-1, 30, 64, 0.5]\n",
        "#mode3 = [0.01, 0.9, 1e-2, 30, 64, 0.5]\n",
        "#mode4 = [0.01, 0.9, 1e-3, 30, 64, 0.5]\n",
        "\n",
        "# Test LR\n",
        "#mode5 = [0.03, 0.9, 0.0, 30, 64, 0.5]\n",
        "#mode6 = [0.005, 0.9, 0.0, 30, 64, 0.5]\n",
        "\n",
        "# random token seqs\n",
        "mode7 = [0.01, 0.9, 0.0, 20, 64, 0.5]\n",
        "\n",
        "\n",
        "list_plots=[]\n",
        "for mode in [mode7]:\n",
        "    lr, momentum, decay, epochs, batch_size, sigthr = mode\n",
        "\n",
        "    variable_data = {\n",
        "        'lr': lr,\n",
        "        'momentum': momentum,\n",
        "        'decay': decay,\n",
        "        'epochs': epochs,\n",
        "        'batch_size': batch_size,\n",
        "        'sigthr': sigthr,\n",
        "        'arch_params': [128,6,3,1,(3, 2, 1),128,10,6,3,1,(3,2,1),70]\n",
        "    }\n",
        "\n",
        "    # Model\n",
        "    model = Net(params=variable_data['arch_params'])\n",
        "    model.to(device)\n",
        "\n",
        "    # Training\n",
        "    train_loss, test_loss, plots = epochs_loop(train_dataloader=train_dataloader,\n",
        "                                              test_dataloader=test_dataloader,\n",
        "                                              model=model,\n",
        "                                              epochs=variable_data['epochs'],\n",
        "                                              lr=variable_data['lr'],\n",
        "                                              momentum=variable_data['momentum'],\n",
        "                                              decay=variable_data['decay'],\n",
        "                                              sig_threshold= variable_data['sigthr'])\n",
        "\n",
        "    list_plots.append(plots)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for plot_buf in list_plots:\n",
        "    img = Image.open(plot_buf)\n",
        "    display.display(img)"
      ],
      "metadata": {
        "id": "Jt4TJr--Wmhf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebookb32539c2d7",
      "provenance": [],
      "gpuType": "T4"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 3881699,
          "sourceId": 7990759,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30839,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}